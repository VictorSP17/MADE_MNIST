{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADE: Masked Autencoder for Distribution Estimation\n",
    "\n",
    "We will apply this algorithm to MNIST dataset for generating new handrwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Hyperparameters...](#1st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='1st'/>\n",
    "\n",
    "## 1. Let us import some libraries and define some classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pylab\n",
    "import torch.distributions.binomial\n",
    "import os\n",
    "from torchvision.utils import save_image\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    \"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n",
    "\n",
    "    # The init gets runned once an object (self) has been assigned this class:)\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        # This initializes the nn.Linear class\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        \n",
    "        # We initialize the mask as ones and it means we are not treating them as parameters!\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "\n",
    "    # This is used to set the masks\n",
    "    def set_mask(self, mask):\n",
    "        self.mask.data.copy_(torch.from_numpy(mask.astype(np.uint8).T))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input.float(), self.mask.float() * self.weight.float(), self.bias.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADE(nn.Module):\n",
    "    def __init__(self, nin, hidden_sizes, nout, num_masks=1, natural_ordering=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        nin: integer; number of inputs\n",
    "        hidden sizes: a list of integers; number of units in hidden layers\n",
    "        nout: integer; number of outputs, which usually collectively parameterize some kind of 1D distribution\n",
    "              note: if nout is e.g. 2x larger than nin (perhaps the mean and std), then the first nin\n",
    "              will be all the means and the second nin will be stds. i.e. output dimensions depend on the\n",
    "              same input dimensions in \"chunks\" and should be carefully decoded downstream appropriately.\n",
    "              the output of running the tests for this file makes this a bit more clear with examples.\n",
    "        num_masks: can be used to train ensemble over orderings/connections\n",
    "        natural_ordering: force natural ordering of dimensions, don't use random permutations\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__() # Initializes nn.Module\n",
    "        self.nin = nin\n",
    "        self.nout = nout\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        \n",
    "        assert self.nout % self.nin == 0, \"nout must be integer multiple of nin\"\n",
    "\n",
    "        # define a simple MLP neural net\n",
    "        self.net = []\n",
    "        hs = [nin] + hidden_sizes + [nout]\n",
    "        for h0,h1 in zip(hs, hs[1:]): # zip function goes through 2 iterable at the same time.\n",
    "            self.net.extend([\n",
    "                    MaskedLinear(h0, h1),\n",
    "                    nn.ReLU(),\n",
    "                ])\n",
    "        self.net.pop() # pop the last ReLU for the output layer\n",
    "        self.net.extend([nn.Sigmoid()])\n",
    "        self.net = nn.Sequential(*self.net)\n",
    "\n",
    "        # seeds for orders/connectivities of the model ensemble\n",
    "        self.natural_ordering = natural_ordering\n",
    "        self.num_masks = num_masks\n",
    "        self.seed = 0 # for cycling through num_masks orderings\n",
    "\n",
    "        self.m = {}\n",
    "        self.update_masks() # builds the initial self.m connectivity\n",
    "        # note, we could also precompute the masks and cache them, but this\n",
    "        # could get memory expensive for large number of masks.\n",
    "\n",
    "    def update_masks(self):\n",
    "        if self.m and self.num_masks == 1: return # only a single seed, skip for efficiency! YES! perfect.\n",
    "        L = len(self.hidden_sizes) # number of layers\n",
    "\n",
    "        # fetch the next seed and construct a random stream\n",
    "        rng = np.random.RandomState(self.seed)\n",
    "        self.seed = (self.seed + 1) % self.num_masks # we repeat the process every num_masks.\n",
    "\n",
    "        # sample the order of the inputs and the connectivity of all neurons\n",
    "        self.m[-1] = np.arange(self.nin) if self.natural_ordering else rng.permutation(self.nin)\n",
    "        for l in range(L):\n",
    "            self.m[l] = rng.randint(self.m[l-1].min(), self.nin-1, size=self.hidden_sizes[l])\n",
    "\n",
    "        # construct the mask matrices\n",
    "        masks = [self.m[l-1][:,None] <= self.m[l][None,:] for l in range(L)]\n",
    "        masks.append(self.m[L-1][:,None] < self.m[-1][None,:])\n",
    "\n",
    "        # handle the case where nout = nin * k, for integer k > 1\n",
    "        if self.nout > self.nin:\n",
    "            k = int(self.nout / self.nin)\n",
    "            # replicate the mask across the other outputs\n",
    "            masks[-1] = np.concatenate([masks[-1]]*k, axis=1)\n",
    "\n",
    "        # set the masks in all MaskedLinear layers\n",
    "        layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n",
    "        for l,m in zip(layers, masks):\n",
    "            l.set_mask(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(split, upto=None):\n",
    "    torch.set_grad_enabled(split=='train') # enable/disable grad for efficiency of forwarding test batches\n",
    "    model.train() if split == 'train' else model.eval()\n",
    "    nsamples = 1 if split == 'train' else args[\"samples\"]\n",
    "    x = xtr if split == 'train' else xte\n",
    "    N,D = x.size() # N is the number of samples and D is the size of each sample\n",
    "    \t\t\t   # In our case 60.000x784 or 10.000x784 are the sizes.\n",
    "    B = 100 # batch size, less than in the loaded code!\n",
    "    nsteps = N//B if upto is None else min(N//B, upto) # enough steps so that we use the whole set\n",
    "    lossfs = []\n",
    "    for step in range(nsteps):\n",
    "\n",
    "        # fetch the next batch of data\n",
    "        xb = Variable(x[step*B:step*B+B])\n",
    "        # xb = x[step*B:step*B+B]\n",
    "        xb = xb.float()\n",
    "\n",
    "        # print(xb.dtype)\n",
    "\n",
    "        # get the logits, potentially run the same batch a number of times, resampling each time\n",
    "        xbhat = torch.zeros_like(xb)\n",
    "        for s in range(nsamples):\n",
    "            # perform order/connectivity-agnostic training by resampling the masks\n",
    "            if step % args[\"resample_every\"] == 0 or split == 'test': # if in test, cycle masks every time\n",
    "                model.update_masks()\n",
    "            # forward the model\n",
    "            xbhat += model(xb)\n",
    "        xbhat /= nsamples\n",
    "\n",
    "        # evaluate the binary cross entropy loss\n",
    "        loss = F.binary_cross_entropy(xbhat, xb, size_average=False) / B # With logits before...\n",
    "        lossf = loss.data.item()\n",
    "        lossfs.append(lossf)\n",
    "\n",
    "        # backward/update\n",
    "        if split == 'train':\n",
    "            opt.zero_grad()\n",
    "            loss.backward()            \n",
    "            opt.step()\n",
    "\n",
    "    print(\"%s epoch average loss: %f\" % (split, np.mean(lossfs)))\n",
    "\n",
    "    if split == 'train':\n",
    "        trainL[epoch] = np.mean(lossfs)\n",
    "\n",
    "    if split == 'test':\n",
    "        testL[epoch] = np.mean(lossfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_64(n):\n",
    "    sample = torch.zeros((64,784))\n",
    "    for r in range(64):\n",
    "        image = np.zeros(28*28)\n",
    "        image = torch.tensor(image)\n",
    "        for i in range(28*28):\n",
    "            # for idx, m in enumerate(model.named_modules()):\n",
    "            #     print(idx, '->', m)\n",
    "            prob = model(image) # Why not outputting between 0 and 1?Â¿?\n",
    "            # print(prob[0:20])\n",
    "            #pixel = np.random.binomial(1, prob[i].detach().numpy())\n",
    "            pixel = torch.round(prob[i]) # clearer image\n",
    "            image[i] = pixel\n",
    "        sample[r] = image\n",
    "    # Now image stores the sampled image using the regular order...\n",
    "    path = 'samples/sample_' + str(n) + '.png'\n",
    "    sample = sample.view(64,1,28,28)\n",
    "    save_image(sample, os.path.join(args[\"sample_dir\"], 'fake_images-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "  'dtr_path': './data/x_train.npy',\n",
    "\n",
    "  'dte_path': './data/x_test.npy',\n",
    "\n",
    "  'hiddens': '500',\n",
    "\n",
    "  'num_masks': 1,\n",
    "\n",
    "  'resample_every': 20,\n",
    "\n",
    "  'samples': 1, \n",
    "    \n",
    "  'epochs': 100,\n",
    "    \n",
    "  'sample_dir': './samples'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters:\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "num_epochs = args[\"epochs\"] # 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Image Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE PREPROCESSING:\n",
    "\n",
    "xtr = np.load(args[\"dtr_path\"])\n",
    "xte = np.load(args[\"dte_path\"])\n",
    "xtr = torch.from_numpy(xtr)\n",
    "xte = torch.from_numpy(xte)\n",
    "\n",
    "# Sizes: 60000x784, 10000x784, recall that 28*28 = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of model parameters: 785284\n"
     ]
    }
   ],
   "source": [
    "# MODEL and optimizer:\n",
    "\n",
    "# Recall, map(fun,iter) applies the function to every element of the iter.\n",
    "hidden_list = list(map(int, args[\"hiddens\"].split(',')))\n",
    "model = MADE(xtr.size(1), hidden_list, xtr.size(1), num_masks=args[\"num_masks\"])\n",
    "print(\"number of model parameters:\",sum([np.prod(p.size()) for p in model.parameters()]))\n",
    "# model.cuda()\n",
    "model = model.float()\n",
    "\n",
    "# set up the optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), 1e-3, weight_decay=1e-4) # Initially -3\n",
    "# Here we apply weight decay to the learning rate, every 45 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=45, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAINING:\n",
    "#state_dict = torch.load('save/model_final.ckpt')\n",
    "#model.load_state_dict(state_dict)\n",
    "\n",
    "testL = np.zeros(num_epochs)\n",
    "trainL = np.zeros(num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(\"epoch %d\" % (epoch, ))\n",
    "    scheduler.step(epoch)\n",
    "    run_epoch('test', upto=5) # run only a few batches for approximate test accuracy\n",
    "    run_epoch('train')\n",
    "    \n",
    "    if (epoch) % 10 == 0:\n",
    "        sample_64(epoch)\n",
    "        torch.save(model.state_dict(), os.path.join('save', 'model--{}.ckpt'.format(epoch+1)))\n",
    "\n",
    "    plt.figure()\n",
    "    pylab.xlim(0, num_epochs + 1)\n",
    "    plt.plot(range(1, num_epochs + 1), trainL, label='train log-likelihood')\n",
    "    plt.plot(range(1, num_epochs + 1), testL, label='test log-likelihood')\n",
    "    plt.legend()\n",
    "    plt.title(\"MADE log-likelihood\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Log-likelihood\")\n",
    "    plt.savefig(os.path.join('save', 'loss.pdf'))\n",
    "    plt.show()\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join('save', 'model_final.ckpt'))\n",
    "print(\"optimization done. full test set eval:\") # 79.72 my last experiment with 100 epochs!\n",
    "if num_epochs > 0:\n",
    "    run_epoch('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEAREST NEIGHBOUR!!!!!\n",
    "# Images 28x28, search the closest one.\n",
    "# function(generated_image) --> closest training_image\n",
    "\n",
    "if NN == True:\n",
    "    aux_data_loader = np.load(args[\"dtr_path\"])\n",
    "\n",
    "    def nearest_gt(generated_image):\n",
    "        min_d = 0\n",
    "        closest = False\n",
    "        for i, image in enumerate(aux_data_loader):\n",
    "            image = np.array(image).reshape(28,28) # all distances in binary...\n",
    "            image = torch.tensor(image).float()\n",
    "            d = torch.dist(generated_image,image) # must be torch tensors (1,28,28)\n",
    "            if i == 0 or d < min_d:\n",
    "                min_d = d\n",
    "                closest = image\n",
    "\n",
    "        return closest\n",
    "\n",
    "    # calculate closest to...\n",
    "    samples = torch.zeros(24, 1, 28, 28)\n",
    "    NN = torch.zeros(24, 1, 28, 28)\n",
    "    for i in range(0,24):\n",
    "            image = torch.tensor(sample(i))\n",
    "            samples[i] = image\n",
    "            NN[i] = nearest_gt(samples[i])\n",
    "            print(i)\n",
    "    save_image(samples, 'f24.png')\n",
    "    save_image(NN.data, 'NN24.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://github.com/karpathy/pytorch-made"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
